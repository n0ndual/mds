* emacs 中运行shell命令 :emacs

1. M-! cmd RET：打开一个名为“*Shell Command Output*“的窗口，并把该命令的执行结果显示在其中。按下”C-x 1“组合键可以关闭这个窗口。由于Shell命令的输出是在一个编辑缓冲区里，因此我们可以对它进行编辑、保存等操作。
M-| cmd RET：运行Shell命令，并使用编辑窗口中选定的区域作为该Shell命令的输入，然后可以选择是否用该Shell命令的输出来替换编辑窗口中选中的区域。
C-u M-! cmd RET：执行一条Shell命令，并将其输出放到编辑区中光标所在的位置处，而不将其输出到”Shell Command Output“窗口。
M-x shell：运行一个子Shell，该子Shell对应于emacs中的一个名为”*Shell*"的缓冲区，此后，我们就可以交互式的运行Shell命令了。
M-x term：运行一个子Shell，该子Shell对应于emacs中的一个名为“＊Terminal*”的缓冲区。使用该命令获得的子Shell是一个完整的Shell的模拟，与我们直接在Shell中操作没有什么差别。
M-x eshell：运行emacs shell。该Shell为emacs自己实现的一个shell，而前面运行的shell都为系统中的shell程序(例如：/bin/csh等）。我们可以通过设置变量shell-file-name来设置emacs所使用的默认shell)"

可以使用C-c + C-c
* 需要学习的emacs package
** ido
** yasnippet
** org
** CMAKE
** ECB
** HELM
** CEDET
** flymake

* 如何安装package，例如ecb
** 1.使用melpa包管理
** 2. 配置init-local.el
例如（require-package 'ecb)

* 不重启更新包
m-x load-file
M-x eval-buffer
* 内存数据库的设计
** 事件驱动框架，实现一个简单server
** 实现简单的数据结构，key-value结构，要考虑到性能
** 实现简单的协议，包括客户端和服务器
** 实现集群功能，master-slave架构或者dht
** 实现rdb或者aof功能（可选）
** 提供一个接口方法，可以故障恢复

* 一致性
lamport算法确保一致性
但是不保证可用性
** 出现n台机器故障时，能否保证一致性

* 可用性
** 停机时间的约束：发生故障时停机多久
** 响应时间的约束：无故障平均响应时间和故障时平均响应时间，响应时间方差
** 从用户的角度看可用性，某个节点挂了后，必然会停机一段时间，直到找到另一个节点。

* 容错性
** 出现n台机器故障时，依然可用，且数据保持一致

* CAP问题的本质
** 容错保障可用性：
n个机器的集群中，发生k个故障时，是否能保证集群不停机，或者在短暂停机后继续可用。是否能保证数据依然是一致的。
** 容错保障一致性：
数据一致性的定义，如果告诉客户某个操作成功，那么最终使该操作在整个集群中成功。

* CAP问题困难的原因：
缺少网络模型和故障模型，使得无法量化可用性和容错性

* CAP是否真的是一个定理：
** 不停机和正常情况最优的可用性和实时一致性是不兼容的
** 不停机和正常情况最优的可用性和最终一致性是兼容的
** 暂停容忍和正常最优可用性和实时一致性是兼容的
** 暂停容忍和正常最优可用性和实时一致性是兼容的

* 网络故障模型？
既要考虑延迟，又要考虑故障导致分区，但没有数学模型；

* 分析：
所有机器都作为proposer和leaner，容错性和一致性很好，但是延迟高。

simple-paxos 正常工作时，延迟是最低的；但存在单点故障；单点故障时重选master；
为了解决单点故障，增加proposer，正常工作时不是最优的；

多个distinguished proposer和distinguished learner的情况下，极大的增加了达成一致需要的时间；增加了正常工作时的延迟；
多个distinguished proposer和distinguished learner的情况下，如果降低至最终一致性，正常情况下的延迟是最优的，而且不存在单点故障。

如何提供一个框架，使得分布式系统开发者可以直接选择自己的CAP策略，是非常有价值的。

* 最佳策略：
** 实现方式
正常工作时使用单master；master故障时，进入全proposer阶段，同时进行重选master；保持了一致性，正常工作时最优，故障时不停机；
故障时，进入全proposer，全learner模式，非常慢；
** 缺点：
master的压力大；每一个读写请求都要与master至少通信一次；
** 优化：
one distinguished proposer, multiple distinguished learner等价于把请求都路由给master

* 框架：
raft算法：
选举算法
master故障时重选master，同时退化到paxos算法；无master的paxos算法是有可能死锁的。
增加或者删除节点时，确保一致性；

现实中的分布式系统不仅仅是一个分布式状态机，而是状态机+计算

* 遗留问题
** 负载均衡与容错？
某一个节点故障后，client如何联系其他节点
mysql数据库的主从复制
mysql数据库的故障恢复
mysql数据库的单点故障
** 永远不停机的情况不存在
因为与client交互的是某一太机器，这台机器故障后，client必须能找到另一台机器来交互。

* 总结
CAP没有意义
设计一个分布式系统，应当先有一个应用或者目标，然后寻找解决方法。
* 继续折腾emacs
** speedbar
** ido
** frames
** smex
** percol
* 终于弄明白怎么杀掉某个进程了
ps aux | percol | awk '{ print $2}' | xargs kill
* 未来工作
** desktop
** theme
* 学习使用c是一个长期的过程，这一次作业先使用go语言
* 分布式系统的驱动力
** 新的应用，deep learning
** 数据集增大
** 新的硬件
* 分布式系统industry problem
  1T data , 100bilian example 10 billian feature
* 目标
** effecient
** flexible concu model
** scala
** fault tolerence
** easy
* parameter server
** p2p / server 两种方式
** server 拆分为 server group

* RDMA
** CPU 50% 网络传输时
** InfiniBand + RDMA ： 共享内存模型
** 延迟非常低，带宽非常高
** 未来会成为网络主流标准。网速大于内存。
** 分布式变得非常简单
** raid buffer, 监控内存是否被别的机器写
** 适用于RDMA架构的模型：
Symmetic Model : pregel ,para server, hadoop 都是
** shared memory api
** locality awareness 利用本地性
** 分布式内存管理：
DHT 每台机器上的内存重新编址
** 内存一致性模型
分布式事务
乐观锁
** FaRM lock-free read

* TAO
** 10倍带宽，50倍延迟
* Consistent Hashing 解决的主要问题
** 扩展性：
一个节点放不下所有的数据，需要多台机器合作
** 扩展快速
随时任意增删节点
** 容错
某节点出错后，尽快找到可用节点
** 负载均衡
所有节点的负载均衡
* Consitent Hashing 的挑战：
** 一致性：
当节点变化时，确保找到正确的节点
** 扩展性：
增删机器时需要做的操作
** 容错
最多能容许多少错误，如何恢复
* zookeeper架构适合于任务并行和数据并行，适合于BSP系统，需要大同步
** master-slave架构
** master由zookeeper来管理
** 如果一个master的压力太大怎么办
* Consistent Hashing 的tradeoff
** 效率换高扩展性
** 虽然降低了每个点的负载，但总负载是增加的
* Consistent Hashing 的实现方式
** hash ring：寻找节点算法
** 加入节点算法：分裂节点算法
** 删除节点算法：合并节点算法
** 一致性算法： 某节点和临近两节点组成主从复制集群
* Consistent Hashing 的一致性模型
** 基本模型是没有容错的，高可用、单点的
* DHT与Consistent Hashing
** DHT补充了一致性模型？
** DHT补充了容错？
* 内存数据库
** 设计
*** redis没有使用DHT，其实应当使用DHT，没有任何不使用DHT的理由
*** 单线程 or 多线程？暂时随意！
*** rdb or aof？主要是aof，定期rdb
*** 每个kv至少两份，往硬盘写一份
** 实现
*** simple server/client
*** consistent hashing 算法：get, set, update, delete
*** 不做节点之间的同步和备份
*** 搞定！
*** 后续工作：一致性模型，增删节点时的操作
* 分布式pagerank设计与实现：
** 设计：
*** master-slave架构：master节点负责分配节点，大同步
*** 故障恢复借鉴RDD
** 实现：
*** dag图
*** 数据复制
*** 中间结果快照
*** 故障恢复

* 假设infiniband成为主流，是否会影响分布式系统的实现方式：
** 影响para server
** 影响RDMA和SM
* QoS网络是否影响分布式系统的实现方式
** 一致性的定义
** CAO理论的理论证明
** 但不影响实现方式
* 6.5听巴赫
** the universe is harmonius
** god exists
** life is enjoyable
** there must be a purpose in life
**
